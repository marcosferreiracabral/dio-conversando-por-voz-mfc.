{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!# Instala√ß√£o correta das depend√™ncias\n",
        "!pip install -U google-generativeai gradio gTTS openai-whisper python-dotenv"
      ],
      "metadata": {
        "id": "PmQeTHsZlD0V",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import whisper\n",
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "from google.api_core import exceptions as google_exceptions\n",
        "from gtts import gTTS\n",
        "import uuid\n",
        "import logging\n",
        "import getpass\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Isso impede que o Python tente sair pelo proxy para falar com o localhost\n",
        "os.environ['NO_PROXY'] = 'localhost,127.0.0.1,0.0.0.0'\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yXSJa47OGTuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configura√ß√£o Inicial ---\n",
        "load_dotenv()\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Autentica√ß√£o\n",
        "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
        "if not api_key:\n",
        "    # Fallback para input manual se n√£o estiver no .env\n",
        "    api_key = getpass.getpass(\"Digite sua Google API Key: \")\n",
        "\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "# --- L√ìGICA DE SELE√á√ÉO DIN√ÇMICA DE MODELO ---\n",
        "def obter_modelo_dinamico():\n",
        "    \"\"\"\n",
        "    Lista os modelos dispon√≠veis na sua conta e seleciona o melhor candidato\n",
        "    automaticamente, evitando erros de 404 por nomes hardcoded.\n",
        "    \"\"\"\n",
        "    print(\"üîç Consultando API do Google para listar modelos dispon√≠veis...\")\n",
        "\n",
        "    try:\n",
        "        # Pega todos os modelos que suportam gerar texto ('generateContent')\n",
        "        todos_modelos = [m for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]\n",
        "\n",
        "        # Ordena prefer√™ncia: Flash (r√°pido) > Pro (inteligente) > Qualquer outro Gemini\n",
        "        # Isso cria uma lista de prioridade baseada no nome\n",
        "        def pontuacao_modelo(m):\n",
        "            nome = m.name.lower()\n",
        "            if '1.5-flash' in nome: return 3\n",
        "            if '1.5-pro' in nome: return 2\n",
        "            if 'gemini' in nome: return 1\n",
        "            return 0\n",
        "\n",
        "        # Ordena a lista pela pontua√ß√£o (do maior para o menor)\n",
        "        todos_modelos.sort(key=pontuacao_modelo, reverse=True)\n",
        "\n",
        "        if not todos_modelos:\n",
        "            raise ValueError(\"Nenhum modelo compat√≠vel encontrado na sua conta Google AI.\")\n",
        "\n",
        "        # Pega o campe√£o\n",
        "        modelo_escolhido = todos_modelos[0]\n",
        "        nome_real = modelo_escolhido.name # Ex: 'models/gemini-1.5-flash-001'\n",
        "\n",
        "        print(f\"‚úÖ Modelo selecionado automaticamente: {nome_real}\")\n",
        "        print(f\"   (Descri√ß√£o: {modelo_escolhido.description})\")\n",
        "\n",
        "        return genai.GenerativeModel(nome_real)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.critical(f\"Falha ao listar modelos. Erro: {e}\")\n",
        "        # √öltimo recurso desesperado: tenta o legacy puro se a listagem falhar\n",
        "        return genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "# Inicializa\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "gemini_model = obter_modelo_dinamico() # <--- Aqui a m√°gica acontece\n",
        "\n",
        "# --- Processamento (O resto segue igual) ---\n",
        "def processar_audio(audio_path):\n",
        "    if audio_path is None:\n",
        "        return \"Nenhum √°udio.\", \"Sem input.\", None\n",
        "\n",
        "    request_id = str(uuid.uuid4())[:8]\n",
        "\n",
        "    # 1. Transcri√ß√£o\n",
        "    try:\n",
        "        audio = whisper_model.transcribe(audio_path, fp16=False)\n",
        "        texto = audio[\"text\"]\n",
        "    except Exception as e:\n",
        "        return f\"Erro Whisper: {e}\", \"\", None\n",
        "\n",
        "    # 2. LLM\n",
        "    try:\n",
        "        logging.info(f\"[{request_id}] Gerando resposta...\")\n",
        "        response = gemini_model.generate_content(texto)\n",
        "        resposta = response.text\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Erro Gemini: {e}\")\n",
        "        resposta = f\"Erro na API: {str(e)}\"\n",
        "\n",
        "    # 3. TTS\n",
        "    path_audio = None\n",
        "    try:\n",
        "        if resposta and not \"Erro\" in resposta:\n",
        "            clean_text = resposta.replace(\"*\", \"\")\n",
        "            tts = gTTS(text=clean_text, lang='pt', slow=False)\n",
        "            path_audio = f\"resp_{request_id}.mp3\"\n",
        "            tts.save(path_audio)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return texto, resposta, path_audio\n",
        "\n",
        "# Interface\n",
        "iface = gr.Interface(\n",
        "    fn=processar_audio,\n",
        "    inputs=gr.Audio(sources=[\"microphone\"], type=\"filepath\"),\n",
        "    outputs=[gr.Textbox(label=\"Voc√™\"), gr.Textbox(label=\"IA\"), \"audio\"],\n",
        "    title=\"Assistente Gemini (Auto-Discovery)\"\n",
        ")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # server_name=\"127.0.0.1\" for√ßa rodar localmente sem tentar abrir para rede\n",
        "  iface.launch(debug=True, server_name=\"127.0.0.1\", server_port=7860, share=False)\n"
      ],
      "metadata": {
        "id": "mOY4GNFJpgvX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}